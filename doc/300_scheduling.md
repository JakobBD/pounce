# Scheduling strategy

There is a considerable number of existing UQ software packages, such as the Dakota toolbox [@Dakota] as the most prominent, or others such as the UQ toolkit [@DebusscherePCE2004; @DebusschereUQTk2017], UQpy [@Olivier2020], PyMLMC [@PyMLMCSukys2017], ChaosPy [@ChaosPyFeinberg2015], UQLab [@Marelli2014], and UQit [@Rezaeiravesh2021b]. Of those that indlue scheduling, most are based on a similar scheduling strategy, which is also depicted in the left-hand part of \autoref{fig:pounce_scheduling}: The framework and thus the whole UQ simulation is carried out within one job. From the framework, external black box codes are executed for sample evaluation. Each sample (i.e. each external program execution) is assigned a number of processors (usually defined once for all samples of that model), such that samples are run partly sequentially and partly in parallel. Since the external codes are treated as a black box, each sample has its own independent file I/O (for clarity, file I/O is omitted in the figure).

![Conventional (left) and PoUnce (right) scheduling strategies. In PoUnce, compute node use time (gray boxes) is optimized by avoiding overhead during placement of program executions (yellow boxes) and by moving stochastic evaluation (blue boxes) out of the compute jobs.\label{fig:pounce_scheduling}](fig/scheduling.pdf)


This strategy entails several performance bottlenecks: 

 * If samples have their independent I/O, this means that the number of files produced by the simulation will be a multiple of the number of samples, which can be in the order of millions. This leads to large numbers of metadata operations, which overwhelms many HPC file systems such as Lustre, which are not designed for largen numbers of files. I/O performance is thus heavly impaired. Apart from that, if a file number quota is given, it is often reached even for small simulations.
 * On some systems, each external sample execution has to be placed on compute nodes by an application level scheduler. This process requires a wall time of several seconds. The coarsest models in a MLMC or MFMC simulation may only require a wall time of less than a second to compute, such that the majority of machine use time is spent on job scheduling instead of computing. In \autoref{fig:pounce_scheduling}, this is indicated by the yellow program-boxes (P), which are wider than the brown sample boxes (S).
 * Executing the UQ framework within a job script means that only one job (J) has to be submitted to the job queue, but it inevitably leads to idle times on most processes during post-processing phases (E), which can be difficult to parallelize. Further, since the number of nodes is fixed for a whole UQ simulation, sample load distribution is rather inflexible. 

A different strategy is thus implemented in PoUnce (shown on the right in \autoref{fig:pounce_scheduling}):

 * PoUnce is not executed within a compute job. For small simulations, it can be executed on a login node of the cluster, but remote capabilities allow it to be executed on any machine with remote access to a cluster with the Secure Shell (SSH) protocol. The framework creates job scripts and submits them to the queue of the cluster. In each iteration of the UQ algorithm, one job is submitted for all samples of the same model. This reduces idle times of processes during post-processing and allows greater flexibility for different parallel load distributions and node counts for different models. A disadvantage of this approach is that repeated queuing times delay the availability of simulation results. 
* The baseline solver may be extended to enable the computation of several samples in one program execution with a common file I/O. The number of total samples in the batch $n_{\text{batch}}$ and the number of parallel samples $n_{\text{parallel}}$ are passed from PoUnce to the adapted solver, and on every process, $n_{\text{sequential}}=\left \lceil{ \frac{n_{\text{batch}}}{n_{\text{parallel}}}}\right \rceil$ samples are executed sequentially in a loop (with some idling processes in the last loop iteration if $n_{\text{batch}}\neq n_{\text{parallel}}n_{\text{sequential}}$). All samples write to the same file. Common parameters are read from a shared ASCII input file, and stochastic parameters (which are different for each sample) are passed in a separate file, which contains their values for all samples. As an example, the adapted baseline flow solver FLEXI uses HDF5 for parallel output and stochastic input. This approach avoids metadata overload of the file system and reduces application level scheduling times. Since it is only semi-intrusive in the sense that the solver has to be modified for this, an option has been retained to launch the samples as separate program executions using GNU parallel. 

